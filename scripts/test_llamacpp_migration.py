#!/usr/bin/env python3
"""
Script para verificar que la migraci√≥n a llama-cpp-python funciona.
"""

import asyncio
import logging
import sys
from pathlib import Path

# Agregar path del proyecto
sys.path.append(str(Path(__file__).parent.parent))

# Configurar logging
logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")

async def test_migration():
    """Prueba completa de la migraci√≥n."""

    print("üîÑ Probando migraci√≥n Ollama ‚Üí LlamaCpp...")

    # Test 1: Importaciones
    try:
        from app.services.llm.llamacpp_provider import LlamaCppProvider
        from app.services.llm.model_downloader import ModelDownloader
        from app.services.llm.llm_manager import LLMManager
        print("‚úÖ Importaciones correctas")
    except ImportError as e:
        print(f"‚ùå Error de importaci√≥n: {e}")
        return False

    # Test 2: Model Downloader
    try:
        downloader = ModelDownloader()
        model_info = downloader.get_model_info()
        print("‚úÖ ModelDownloader funciona")
        print(f"   Modelos configurados: {list(model_info.keys())}")
    except Exception as e:
        print(f"‚ùå Error en ModelDownloader: {e}")
        return False

    # Test 3: Provider b√°sico
    try:
        provider = LlamaCppProvider()
        print("‚úÖ LlamaCppProvider instanciado")

        # Verificar disponibilidad (sin cargar modelos)
        try:
            import llama_cpp
            print("‚úÖ llama-cpp-python est√° instalado")
        except ImportError:
            print("‚ùå llama-cpp-python NO est√° instalado")
            print("   Ejecuta: pip install llama-cpp-python==0.2.77")
            return False

    except Exception as e:
        print(f"‚ùå Error en LlamaCppProvider: {e}")
        return False

    # Test 4: Mock de BokiApi para LLMManager
    class MockBokiApi:
        async def close(self):
            pass

    try:
        llm_manager = LLMManager(MockBokiApi())
        print("‚úÖ LLMManager instanciado correctamente")
    except Exception as e:
        print(f"‚ùå Error en LLMManager: {e}")
        return False

    # Test 5: Descarga de modelos (opcional)
    print("\nü§î ¬øQuieres probar la descarga de modelos? (tardar√° varios minutos)")
    print("   Escribe 'y' para descargar, cualquier otra cosa para saltar:")

    try:
        user_input = input().strip().lower()
        if user_input == 'y':
            print("üì• Iniciando descarga de modelos...")

            # Descargar solo modelo peque√±o para testing
            success, path = await downloader.ensure_model_available("intent")

            if success:
                print(f"‚úÖ Modelo de intenciones descargado: {path}")
                print("‚úÖ ¬°Migraci√≥n completamente funcional!")
            else:
                print("‚ùå Error descargando modelo")
                return False
        else:
            print("‚è≠Ô∏è Saltando descarga de modelos")
            print("‚úÖ Migraci√≥n lista (modelos se descargar√°n autom√°ticamente)")

    except KeyboardInterrupt:
        print("\n‚è≠Ô∏è Saltando descarga de modelos")

    print("\nüéâ ¬°Migraci√≥n a llama-cpp-python completada!")
    print("\nüìã Pr√≥ximos pasos:")
    print("   1. Ejecutar: uvicorn app.main:app --reload")
    print("   2. Enviar mensaje de prueba al webhook")
    print("   3. Los modelos se descargar√°n autom√°ticamente en primer uso")

    return True

if __name__ == "__main__":
    success = asyncio.run(test_migration())
    sys.exit(0 if success else 1)